{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model selection and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings;warnings.filterwarnings('ignore');import matplotlib.pyplot as plt;import numpy as np;import pandas as pd;import seaborn as sns;sns.set_context(\"notebook\", font_scale=1.4);sns.set_style(\"whitegrid\");import imp;compomics_import = imp.load_source('compomics_import', '../compomics_import.py');from IPython.core.display import HTML;css_file = '../my.css';HTML(open(css_file, \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model selection and validation** are fundamental steps in statistical learning applications. In particular, we wish to compute (select) the model that performs optimally for **unseen external data** (data not seen during fitting the model parameters). For instance, when we augment the features in a data set by polynomial features of a certain degree $d$ we need to set $d$ such that it allows for fitting a model that performs best on all unseen external data. \n",
    "Parameters such as $d$ that needs to be set by the user are known as a **hyperparameters**.\n",
    "\n",
    "Model selection is about the following considerations:\n",
    "\n",
    "- when is a model a good model (e.g. what metric to use to validate the generalization performance of the model)?\n",
    "- how to best pre-process the data (e.g. how to best normalize the data)?\n",
    "- what to assume about the true model underlying the data (e.g linear/non-linear)?\n",
    "- what are good values for the hyperparameters (this is known as **model tuning**)?\n",
    "\n",
    "All these decisssions have to be made such that the resulting prediction model performs well on **unseen data**. \n",
    "\n",
    "The issue is of course that we don't have access yet to the unseen data. So we have to create it. We can estimate the expected performance on unseen data by splitting the data set into a data set for fitting (the **train set**) and a data set for testing the model (the **test set**). The process of fitting a model on a data set is also known as **training**.   \n",
    "\n",
    "The testing data should **never** be used to make decission about what model to select.\n",
    "\n",
    "Let's load a regression dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "sns.lmplot(x=\"x1\", y=\"y\", data=dataset, fit_reg=False, height=5.5, scatter_kws={\"s\": 80})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly select 50% of the data set that we will treat as the unseen external data, known as the **(independent) test set**. The other 50% data points will be used for fitting (or **training**) a model and is known as the **train set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "dataset['folds'] = np.random.randint(2,size=len(dataset))\n",
    "\n",
    "sns.lmplot(x=\"x1\", y=\"y\", col=\"folds\", \n",
    "           data=dataset, fit_reg=False, height=5.5, scatter_kws={\"s\": 80})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will augment the feature vectors with polynomial features of various degrees $d$ and fit a linear regression model to each train set. We evaluate the prediction performance on the corresponding test set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "feature_scaler = MinMaxScaler()\n",
    "model = LinearRegression()\n",
    "\n",
    "X = dataset.copy()\n",
    "y = X.pop('y')\n",
    "folds = X.pop('folds')\n",
    "\n",
    "train_scores = [0] #no features (d=0)\n",
    "test_scores = [0]\n",
    "\n",
    "model.fit(X[folds!=1],y[folds!=1])\n",
    "train_scores.append(model.score(X[folds!=1],y[folds!=1])) # d=1\n",
    "test_scores.append(model.score(X[folds==1],y[folds==1]))\n",
    "\n",
    "for degree in range(2,18,1):\n",
    "    X['x1^'+str(degree)] = X['x1']**degree\n",
    "    X['x1^'+str(degree)] = feature_scaler.fit_transform(X[ ['x1^'+str(degree)]])\n",
    "    model.fit(X[folds!=1],y[folds!=1])\n",
    "    train_scores.append(model.score(X[folds!=1],y[folds!=1]))\n",
    "    test_scores.append(model.score(X[folds==1],y[folds==1]))\n",
    "\n",
    "tmp = pd.DataFrame()\n",
    "tmp['train-scores'] = train_scores\n",
    "tmp['test-scores'] = test_scores\n",
    "tmp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue curve shows how the accuracy on the train set increases with $d$ while the green curve shows how the accuracy on the test set starts to decrease for large values of $d$. Since both the train and test set are drawn randomly from the same data set we can conclude that $d>3$ allowed the linear regression algorithm to **fit the noise** in the augmented train set. We call this **overfitting**. For $d<3$ we can conclude that the model is not complex enough to fit the data set accurately. We call this **underfitting**.\n",
    "\n",
    "It is not easy to avoid overfitting. For instance, to find the optimal value for $d$ we could train models with different values for $d$ on the train set and select the setting that performs best on the test set. We could then report the accuracy on this test set as an estimate of the performance of the model. However, in this case the test set would not represent true unseen external data as it is used to decide on the optimal value for $d$. As consequence the reported estimated generalization performance can be overly optimistic and not reflect the true expected generalization performance on the uneen external data.\n",
    "\n",
    "To solve this problem, yet another part of the data set should be held out (known as the **validation set**). In this case a model is fitted on the train set and the optimal value for $d$ is selected using the validation set. A model is then trained on the union of the train and validation set using the best value for $d$ and the performance of this model is then tested on the test set to estimate the generalization performance. \n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for training and evaluating the model. Especially for smaller data sets this means that not only the fit of the model (train set) but also the decision on the values of the hyperparameters (validation set) as well as the estimation of the generalization performance (test set) will depend strongly on a particular random choice of the data set splits. \n",
    "\n",
    "To make our conclusions less dependent on the choice of the data split we can perform an approach called **cross-validation** (CV). In the basic setup, called **$k$-fold CV**, the data set is split into $k$ disjoint sets of approximately equal size. The following procedure is followed for each of the $k$ “folds” $D_i$ ($i=1 \\ldots k$):\n",
    "\n",
    "1. train a model $m_i$ on the data points in folds $D_j$ with $j \\ne i$,\n",
    "2. use $m_i$ to compute predictions for $D_i.$\n",
    "\n",
    "As you can see we now have one prediction for each data point such that the model that computed that prediction was not trained on a train set that contained that data point. The performance measure reported by k-fold cross-validation is computed using these predictions and constitutes a good estimate for the generalization performance of the model. At the same time by choosing larger values for $k$ (typical value is 10) the size of the train set becomes larger and better models can be fitted, while still computing one prediction for each data point. \n",
    "\n",
    "Let's look at an implementation of the cross-validation loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cross_val_predict(model,X,y,cv=5):\n",
    "    predictions = np.empty(len(y))\n",
    "    folds = np.random.randint(0,cv,size=len(y))\n",
    "    for i in range(cv):\n",
    "        Xtest = X[folds==i]\n",
    "        ytest = y[folds==i]\n",
    "        Xtrain = X[folds!=i]\n",
    "        ytrain = y[folds!=i]\n",
    "        model.fit(Xtrain,ytrain)\n",
    "        p = model.predict(Xtest)\n",
    "        counter = 0\n",
    "        for j in range(len(folds)):\n",
    "            if folds[j] == i:\n",
    "                predictions[j] = p[counter]\n",
    "                counter += 1            \n",
    "    return predictions\n",
    "        \n",
    "X = dataset[dataset['folds']==0].copy()\n",
    "y = X.pop('y')    \n",
    "folds = X.pop('folds')\n",
    "\n",
    "cv_predictions = my_cross_val_predict(model,X,y,cv=5)\n",
    "\n",
    "print(cv_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute an estimate of the generalization performance of the model using these predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.r2_score(y,cv_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn we can use the `cross_val_predict()` function in the module `sklearn.cross_validation` to do the same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "cv_predictions = cross_val_predict(model,X,y,cv=5)\n",
    "print(cv_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.r2_score(y,cv_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the degree $d$ we can now use the CV loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[dataset['folds']==0].copy()\n",
    "y = X.pop('y')\n",
    "folds = X.pop('folds')\n",
    "\n",
    "scores = []\n",
    "scores.append(0) # no features (d=0)\n",
    "cv_predictions = cross_val_predict(model, X, y, cv=5) # (d=1)\n",
    "scores.append(metrics.r2_score(y,cv_predictions)) \n",
    "for degree in range(2,18,1):\n",
    "    X['x1^'+str(degree)] = X['x1']**degree\n",
    "    X['x1^'+str(degree)] = feature_scaler.fit_transform(X[ ['x1^'+str(degree)]])\n",
    "    cv_predictions = cross_val_predict(model, X, y, cv=5) # (d=1)\n",
    "    scores.append(metrics.r2_score(y,cv_predictions)) \n",
    "\n",
    "tmp = pd.DataFrame(scores,columns=['cv_score'])\n",
    "tmp.plot(ylim=(0,1))\n",
    "plt.show()\n",
    "print(\"best CV score: {}\".format(max(scores)))\n",
    "print(\"optimal degree d: {}\".format(np.argmax(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we consider this best CV score a good estimated of the generalization performance of a model with optimal degree $d$? No. The CV predictions were used to make the decision on the optimal degree $d$. Instead we need a second CV loop that is nested in the first loop to estimate the generalization performance while minimizing the possibility of overfitting. This is called **nested-CV** and will be explained further in this course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
